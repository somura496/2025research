{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "huggingface(pt) + WRIME で感情推定.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# huggingface + WRIMEデータセット で感情推定"
      ],
      "metadata": {
        "id": "MCWkFr6WCXOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 環境構築"
      ],
      "metadata": {
        "id": "QMFt91C2hSxY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw--A4s2hGh6",
        "outputId": "ca4e7c77-53da-40eb-8ddc-3416481b488f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: fugashi in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: ipadic in /usr/local/lib/python3.12/dist-packages (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "# huggingface transformer のインストール\n",
        "# - transformers : 主たるモジュール（モデルやトークナイザのダウンロード)\n",
        "# - datasets : HuggingFaceで、データセットを扱うためのモジュール\n",
        "# https://huggingface.co/docs/transformers/installation\n",
        "! pip install transformers datasets\n",
        "\n",
        "# 東北大学の日本語用BERT使用に必要なパッケージをインストール\n",
        "! pip install fugashi ipadic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "VPbUMvlMjR0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face (Transformers) 関連のモジュール\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import Dataset, load_metric"
      ],
      "metadata": {
        "id": "8-t2MF0PC4ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matplotlibで日本語を使用できるようにする"
      ],
      "metadata": {
        "id": "9g5Irgf0VgHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [前準備] Matplotlib で日本語フォントを使用できるようにする\n",
        "# cf. https://blog.3qe.us/entry/2018/08/16/121457\n",
        "!apt-get -y install fonts-ipafont-gothic\n",
        "!rm /root/.cache/matplotlib/fontlist-v310.json\n",
        "\n",
        "# NOTE ここで、ランタイムを再起動"
      ],
      "metadata": {
        "id": "wZr0sIJqVkHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(font='IPAGothic')\n",
        "\n",
        "# 動作確認\n",
        "plt.figure(figsize=(5,1))\n",
        "plt.title('日本語を表示できるかテスト')"
      ],
      "metadata": {
        "id": "yyTglWfVVq72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセットの準備"
      ],
      "metadata": {
        "id": "fL1vTBc2hxDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WRIMEデータセットのダウンロード"
      ],
      "metadata": {
        "id": "t1-mna7wwHAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GitHubよりWRIMEデータをダウンロードする\n",
        "#\n",
        "# WRIME dataset : https://github.com/ids-cv/wrime\n",
        "# 今回使用するのは ver1 （感情極性が付与されていない版）\n",
        "! wget https://github.com/ids-cv/wrime/raw/master/wrime-ver1.tsv"
      ],
      "metadata": {
        "id": "O9vmzOBthkTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pandas.DataFrameとして読み込む\n",
        "df_wrime = pd.read_table('wrime-ver1.tsv')\n",
        "df_wrime.head(2)"
      ],
      "metadata": {
        "id": "p5xtl0bLh3sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 前処理"
      ],
      "metadata": {
        "id": "IHyn6u4UEelB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__前提①：WRIMEデータセットのラベルは、各感情の強度__\n",
        "- 各感情（例えば、Joy=喜）の強度を、0〜3の４段階でラベル付けしている\n",
        "- ８つの感情全てにおいて、最頻ラベルは「0」\n",
        "\n",
        "\n",
        "__前提②：WRIMEデータセットには、複数種類のラベルが付与されている__\n",
        "1. Writer_*\n",
        "    - 文章の __書き手__ が自身で付与したラベル。「主観感情」。\n",
        "2. Reader{1,2,3}_* :\n",
        "    - 文章の __読み手__ が付与したラベル。「客観感情」。３名分。\n",
        "3. Avg.Readers_*\n",
        "    - ３名分の客観感情の平均値。"
      ],
      "metadata": {
        "id": "EahRxhWvDd82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "問題設定\n",
        "1. ８つの感情の、分類タスクとして扱う\n",
        "    - 相対的にどの感情が強いかを推定する\n",
        "    - データセットの本来の用途としては感情強度を推定するタスク。しかしながら、感情強度=0のサンプルが多く、やや扱いが難しいため、今回は簡素化して扱う。\n",
        "2. 客観感情の平均値を使用する\n",
        "    - 論文において、主観感情と客観感情は異なることが指摘されている\n",
        "    - 主観感情は、書き手の性格や表現方法に依存する部分がある。そのため、客観感情、かつ、その平均値を用いることで、推定結果の納得感が高くなることが期待される"
      ],
      "metadata": {
        "id": "RqlORat4Emxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plutchikの8つの基本感情\n",
        "emotion_names = ['Joy', 'Sadness', 'Anticipation', 'Surprise', 'Anger', 'Fear', 'Disgust', 'Trust']\n",
        "emotion_names_jp = ['喜び', '悲しみ', '期待', '驚き', '怒り', '恐れ', '嫌悪', '信頼']  # 日本語版\n",
        "num_labels = len(emotion_names)\n",
        "\n",
        "# readers_emotion_intensities 列を生成する\n",
        "# \"Avg. Readers_*\" の値をlist化したもの\n",
        "df_wrime['readers_emotion_intensities'] = df_wrime.apply(lambda x: [x['Avg. Readers_' + name] for name in emotion_names], axis=1)\n",
        "\n",
        "# 感情強度が低いサンプルは除外する\n",
        "# (readers_emotion_intensities の max が２以上のサンプルのみを対象とする)\n",
        "is_target = df_wrime['readers_emotion_intensities'].map(lambda x: max(x) >= 2)\n",
        "df_wrime_target = df_wrime[is_target]"
      ],
      "metadata": {
        "id": "DeWD5aJGwWYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train / test に分割する\n",
        "df_groups = df_wrime_target.groupby('Train/Dev/Test')\n",
        "df_train = df_groups.get_group('train')\n",
        "df_test = pd.concat([df_groups.get_group('dev'), df_groups.get_group('test')])\n",
        "print('train :', len(df_train))\n",
        "print('test :', len(df_test))"
      ],
      "metadata": {
        "id": "aUzRUtnhxW83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデル(BERT)を訓練する"
      ],
      "metadata": {
        "id": "4Nic3TqEx_Lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizerで入力データに変換"
      ],
      "metadata": {
        "id": "lfue7lSvJeKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用するモデルを指定して、Tokenizerを読み込む\n",
        "checkpoint = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "wHHNGwi9ymSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 前処理関数: tokenize_function\n",
        "# 感情強度の正規化（総和=1）も同時に実施する\n",
        "def tokenize_function(batch):\n",
        "    tokenized_batch = tokenizer(batch['Sentence'], truncation=True, padding='max_length')\n",
        "    tokenized_batch['labels'] = [x / np.sum(x) for x in batch['readers_emotion_intensities']]  # 総和=1に正規化\n",
        "    return tokenized_batch\n",
        "\n",
        "# Transformers用のデータセット形式に変換\n",
        "# pandas.DataFrame -> datasets.Dataset\n",
        "target_columns = ['Sentence', 'readers_emotion_intensities']\n",
        "train_dataset = Dataset.from_pandas(df_train[target_columns])\n",
        "test_dataset = Dataset.from_pandas(df_test[target_columns])\n",
        "\n",
        "# 前処理（tokenize_function） を適用\n",
        "train_tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_tokenized_dataset = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "uQnEu-ZZNFcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 訓練済みモデルの読み込み"
      ],
      "metadata": {
        "id": "Sl6xWI-wQ_kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 分類モデルのため AutoModelForSequenceClassification を使用する\n",
        "# checkpoint と num_labels（クラス数） を指定する. 今回は、いずれも上で定義済み\n",
        "# - checkpoint = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
        "# - num_labels = 8\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)"
      ],
      "metadata": {
        "id": "QQyrGO2XzPYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 訓練を実行"
      ],
      "metadata": {
        "id": "BwIj00p0myL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 評価指標を定義\n",
        "# https://huggingface.co/docs/transformers/training\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    label_ids = np.argmax(labels, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=label_ids)"
      ],
      "metadata": {
        "id": "8lrhTIfwzos6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers の Trainer を用いる\n",
        "# https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/trainer#transformers.TrainingArguments\n",
        "\n",
        "# 訓練時の設定\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=1.0,\n",
        "    evaluation_strategy=\"steps\", eval_steps=200)  # 200ステップ毎にテストデータで評価する\n",
        "\n",
        "# Trainerを生成\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized_dataset,\n",
        "    eval_dataset=test_tokenized_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 訓練を実行\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "2noasiZazsSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 訓練したモデルで推論する"
      ],
      "metadata": {
        "id": "IfvBTX991SW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.delftstack.com/ja/howto/numpy/numpy-softmax/\n",
        "def np_softmax(x):\n",
        "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
        "    return f_x\n",
        "\n",
        "def analyze_emotion(text, show_fig=False, ret_prob=False):\n",
        "    # 推論モードを有効か\n",
        "    model.eval()\n",
        "\n",
        "    # 入力データ変換 + 推論\n",
        "    tokens = tokenizer(text, truncation=True, return_tensors=\"pt\")\n",
        "    tokens.to(model.device)\n",
        "    preds = model(**tokens)\n",
        "    prob = np_softmax(preds.logits.cpu().detach().numpy()[0])\n",
        "    out_dict = {n: p for n, p in zip(emotion_names_jp, prob)}\n",
        "\n",
        "    # 棒グラフを描画\n",
        "    if show_fig:\n",
        "        plt.figure(figsize=(8, 3))\n",
        "        df = pd.DataFrame(out_dict.items(), columns=['name', 'prob'])\n",
        "        sns.barplot(x='name', y='prob', data=df)\n",
        "        plt.title('入力文 : ' + text, fontsize=15)\n",
        "\n",
        "    if ret_prob:\n",
        "        return out_dict\n",
        "\n",
        "# 動作確認\n",
        "analyze_emotion('今日から長期休暇だぁーーー！！！', show_fig=True)"
      ],
      "metadata": {
        "id": "fSFeecw21fjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_emotion('この書類にはコーヒーかかってなくて良かった…。不幸中の幸いだ。', show_fig=True)"
      ],
      "metadata": {
        "id": "_aolL3TAJImq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_emotion('なんで自分だけこんな目に遭うんだ……', show_fig=True)"
      ],
      "metadata": {
        "id": "2Wg2978fMB6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_emotion('君ならきっとやってくれると思っていたよ！', show_fig=True)"
      ],
      "metadata": {
        "id": "Bg0YRm0vMN26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_emotion('え、今日って休校だったの？', show_fig=True)"
      ],
      "metadata": {
        "id": "hVNf1bmAMVYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_emotion('明日のプレゼンうまくできるかなぁ…', show_fig=True)"
      ],
      "metadata": {
        "id": "9JADJ8eaMaHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_emotion('あぁー、イライラするっ！！', show_fig=True)"
      ],
      "metadata": {
        "id": "hz0a58-62a6J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}